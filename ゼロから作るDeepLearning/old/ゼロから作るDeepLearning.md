<!-- $theme: gaia -->
<!-- $size: 4:3 -->
<!-- prerender: true -->
<!-- page_number: true -->
<!-- <!-- footer: ゼロから作るDeepLearning 学習ノート-->

# ゼロから作るDeepLearning 学習ノート

---
## 前提

この学習ノートでは以下列挙する事項を前提とする。

- 行列計算を習った記憶がわずかにあること。
- サンプルコードとしてPythonを使用する。
  - プログラムの雰囲気がわかればよい。
  - 行列計算にnumpyといライブラリを使う。
- 結局は役に立たない。
  - tensorflowなどのライブラリがやる。

---
## Contens

1. パーセプトロン
1. ニューラルネットワーク
1. ニューラルネットワークの学習
1. 逆誤差伝播方式
1. 学習に関するテクニック
1. 畳み込みニューラルネットワーク
1. ディプラーニング

---
## パーセプトロン

---
### パーセプトロンとは

パーセプトロンは、複数の信号を入力として受け取り、ひとつの信号を出力する。

例：２つの入力を受け取るパーセプトロン。

![100% center パーセプトロンのイメージ](./01_perceptron/perceptron.png)

$$
y = 
  \begin{cases}
    0 & (w_1 x_1 + w_2 x_2 + b<= 0)\\
    1 & (w_1 x_1 + w_2 x_2 + b>  1)
  \end{cases}
$$

---
#### パセープトロンで使用する変数

$$
y = 
  \begin{cases}
    0 & (w_1 x_1 + w_2 x_2 + b<= 0)\\
    1 & (w_1 x_1 + w_2 x_2 + b>  1)
  \end{cases}
$$

変数|説明
:--|:--
$x_1$, $x_2$ | 入力値
$w_1$, $w_2$ | 各入力値に対する重み
$b$| 出力値を決定するための閾値
$y$ | 出力値

---
### 簡単な実装（ORゲート）

$w_1 = 0.5$, $w_2 = 0.5$, $b = -0.2$の重みを持つパーセプトロンで、ORゲートを実現できる。

$$
y = 
  \begin{cases}
    0 & (0.5 x_1 + 0.5 x_2 - 0.2 <= 0)\\
    1 & (0.5 x_1 + 0.5 x_2 - 0.2 >  1)
  \end{cases}
$$

$x_1$ | $x_2$ | $y$ | |
--:|--:|--:|--:
0 | 0 | 0 | (-0.2)
0 | 1 | 1 | ( 0.3)
1 | 0 | 1 | ( 0.3)
1 | 1 | 1 | ( 0.8)

---
#### パーセプトロンの実装イメージ（ORゲート）

```python
def OR(x1, x2):
    x = np.array([x1, x2])
    w = np.array([0.5, 0.5])
    b = -0.2
    tmp = np.sum(w * x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

#### 実行結果（ORゲート）

```python
print(OR(0, 0)) -> 0
print(OR(0, 1)) -> 1
print(OR(1, 0)) -> 1
print(OR(1, 1)) -> 1
```

---
### 簡単な実装（ANDゲート）

$w_1 = 0.5$, $w_2 = 0.5$, $b = -0.7$の重みを持つパーセプトロンで、ANDゲートを実現できる。

$$
y = 
  \begin{cases}
    0 & (0.5 x_1 + 0.5 x_2 - 0.7 <= 0)\\
    1 & (0.5 x_1 + 0.5 x_2 - 0.7 >  1)
  \end{cases}
$$

$x_1$ | $x_2$ | $y$ | |
--:| --:|--:|--:
 0 |  0 | 0 | (-0.7)
 0 |  1 | 0 | (-0.2)
 1 |  0 | 0 | (-0.2)
 1 |  1 | 0 | ( 0.3)

---
#### パーセプトロンの実装イメージ（ANDゲート）

```python
def AND(x1, x2):
    x = np.array([x1, x2])
    w = np.array([0.5, 0.5])
    b = -0.7
    tmp = np.sum(x * w) + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

#### 実行結果（ANDゲート）

```python
print(AND(0, 0)) -> 0
print(AND(0, 1)) -> 0
print(AND(1, 0)) -> 0
print(AND(1, 1)) -> 1
```

---
### 簡単な実装（NANDゲート）

$w_1 = -0.5$, $w_2 = -0.5$, $b = 0.7$の重みを持つパーセプトロンで、NANDゲートを実現できる。

$$
y = 
  \begin{cases}
    0 & (-0.5 x_1 - 0.5 x_2 + 0.7 <= 0)\\
    1 & (-0.5 x_1 - 0.5 x_2 + 0.7 >  1)
  \end{cases}
$$

$x_1$ | $x_2$| $y$ ||
--:|--:|--:|--:
0 | 0 | 1 | ( 0.7)
0 | 1 | 1 | ( 0.2)
1 | 0 | 1 | ( 0.2)
1 | 1 | 0 | (-0.3)

---
#### パーセプトロン実装イメージ（NANDゲート）

```python
def NAND(x1, x2):
    x = np.array([x1, x2])
    w = np.array([-0.5, -0.5])
    b = 0.7
    tmp = np.sum(w * x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

#### 実行結果（NANDゲート）

```python
print(NAND(0, 0)) -> 1
print(NAND(0, 1)) -> 1
print(NAND(1, 0)) -> 1
print(NAND(1, 1)) -> 0
```

---
### パーセプトロンの特徴

- 重みをの値を変更するだけで、様々な表現をすることができる。
- 論理回路を使った例では、入力が２信号だけだったが、もっとたくさんの入力信号があっても良い。１００でも２００でも。

---
### 簡単な実装？（XORゲート）

XORゲートは下表のような論理回路となる。

$x_1$ | $x_2$ | $y$
--:|--:|--:
0 | 0 | 0
0 | 1 | 1
1 | 0 | 1
1 | 1 | 0

これを実現するパーセプトロンの重み$w_1$, $w_2$,や閾値$b$にはどのような値を用意すればよいか。

---
#### パーセプトロンで表現できない非線形

パーセプトロンでは、XORゲートを表現できない。
パーセプトロンは、１本の直線で領域を分離可能なものを表現できるが、そうでないものを表現できない。
![70%](./02_neural_network/graph_or.png) ![70%](./02_neural_network/graph_and.png)![70%](./02_neural_network/graph_nand.png)
![70% center](./02_neural_network/graph_xor.png)

---
#### 多層パーセプトロンによる実現(XOR)

ひとつのパーセプトロンでXORゲートを表現できなくても、これまでに作成したNANDゲート、ORゲート、ANDゲートを組み合わせればXORゲートも表現可能。

![100% center XORゲートを実現する論理回路](./01_perceptron/xor_gate.png)

---
#### 多層パーセプトロン実装イメージ(XORゲート)

```python
def XOR(x1, x2):
    s1 = NAND(x1, x2)
    s2 = OR(x1, x2)
    y = AND(s1, s2)
    return y
```

#### 実行結果（XORゲート）

```python
print(XOR(0, 0)) -> 0
print(XOR(0, 1)) -> 1
print(XOR(1, 0)) -> 1
print(XOR(1, 1)) -> 0
```

---
### まとめ

- パーセプトロンは、ある入力を与えると決まった値が出力される。
- パーセプトロンは、「重み」と「閾値」をパラメータとして設定する。
- 単層のパーセプトロンは線形領域だけしか表現できない。
- 多層のパーセプトロンでは非線形領域を表現することができる。
- 論理回路を表現できれば大量のパーセプトロンで、パソコンでできることは全てできるはず…
---
## ニューラルネットワーク

---
### パーセプトロンの復習

パーセプトロンは下図のようなイメージ。

![100% center パーセプトロンのイメージ](./02_neural_network/perceptron.png)

以下のような数式で示すことができた。

$$
y = 
  \begin{cases}
    0 & (w_1 x_1 + w_2 x_2 + b <= 0)\\
    1 & (w_1 x_1 + w_2 x_2 + b >  1)
  \end{cases}
$$

---
#### パーセプトロンの整理
パーセプトロンを「入力信号の総和を求める部分」と「総和から出力値を求める部分」に分解する。

![70% center パーセプトロンに活性化関数を導入](./02_neural_network/activation_function.png)

数式で表すと下式のようになる。
$$
\begin{aligned}
a &= w_1 x_1 + w_2 x_2 + b \\
y &= h(a)
\end{aligned}
$$

---
### 活性化関数

$h(x)$の部分を活性化関数と呼ぶ。

パーセプトロンでは、活性化関数に下式のようなステップ関数を使用していた。

$$
h(x) = \begin{cases}
    0 & (x <= 0)\\
    1 & (x >  1)
  \end{cases}
$$

ニューラルネットワークでは、活性化関数にシグモイド関数を使うのが一般的。ディープラーニングでは、ReLU関数やthanh関数を用いることが多い。

---
#### 活性化関数（ステップ関数）

パーセプトロンで使用していた活性化関数。
入力値が$0$以上であれば$１$を、入力値が$0$未満であれば$0$を出力する。

$$
h(x) = \begin{cases}
    0 & (x <= 0)\\
    1 & (x >  1)
  \end{cases}
$$


---
##### 活性化関数（ステップ関数：実装）


ステップ関数を実装する。
```python
def step_function(x)
    return np.array(x > 0, dtype=npint)
```

ステップ関数の実行結果をグラフでプロット。
![100% center ステップ関数のグラフ](./02_neural_network/step_function.png)

---
#### 活性化関数（シグモイド関数）

シグモイド関数は、ニューラルネットワークでよく用いられる関数のひとつ。
シグモイド関数は下式のとおり。

$$
h(x) = \frac{1}{1 + exp(-x)}
$$

式中の$exp(-x)$は、$e^{-x}$を意味する($e$はネイピア数の実数$2.7182...$を表す)。

パーセプトロンとニューラルネットワークの違いは、活性化関数にステップ関数を使用しているか、シグモイド関数を使用しているかの差だけ。


---
##### 活性化関数(シグモイド関数：実装)

シグモイド関数を実装する。
```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

シグモイド関数の実行結果をグラフでプロット。
![100% center シグモイド関数のグラフ](./02_neural_network/sigmoid_function.png)


---
##### 活性化関数(シグモイド関数：つかう理由)

![100% ステップ関数のグラフ](./02_neural_network/step_function.png)![100% シグモイド関数のグラフ](./02_neural_network/sigmoid_function.png)

ニューラルネットワークで、ステップ関数ではなくシグモイド関数を活性化関数に使用するのは、シグモイド関数が入力値に対して連続的に出力値が変化する性質を持つため。
この滑らかさが、後述するニューラルネットワークの学習において重要な意味を持つ。

---
#### 活性化関数（ReLU関数）

シグモイド関数は、ニューラルネットワークの歴史において古くから利用されていた。現在では、ReLU関数(Rectified Linear Unit)が利用されることが多い。

$$
h(x) = \begin{cases}
    x & (x > 0)\\
    0 & (x <= 0)
  \end{cases}
$$

---
##### 活性化関数(ReLU関数：実装)

ReLU関数を実装する。
```python
def relu(x) :
    return np.maximum(0, x)
```

ReLU関数の実行結果をグラフでプロット。
![100% center ReLU関数のグラフ](./02_neural_network/relu_function.png)

---
### ３層ニューラルネットワーク

３層ニューラルネットワークのイメージを下に示す。

![70% center ３層ニューラルネットワークのイメージ](./02_neural_network/neural_network.png)

入力が２つ($x_1$と$x_2$)に、隱れ層が２層、出力が２つ($y_1$と$y_2$)となっている。

---
#### ３層ニューラルネットワーク(変数のお約束)

３層ニューラルネットーワクの計算をみる前に、変数の添字にルールを決める。

変数 | 意味
---|---
$x_n$      | $n$個目の入力値
$w^i_{nm}$ | $i-1$層目$m$個目から $i$層目$n$個目への重み
$b^i_{n}$  | $i$層目$n$個目の閾値
$a^i_n$    | $i$層目$n$個目の入力値総和
$z^i_n$    | $i$層目$n$個目の出力値
$y_n$      | $n$個目の出力値


---
#### ３層ニューラルネットワーク(隱れ層①)

入力層からの入力から隱れ層①の出力値を計算。

$$
\begin{aligned}
a^1_1 &= w^1_{11} x_1 + w^1_{12} x_2 + b^1_1 \\
a^1_2 &= w^1_{21} x_1 + w^1_{22} x_2 + b^1_2 \\
a^1_3 &= w^1_{31} x_1 + w^1_{32} x_2 + b^1_3
\end{aligned}
$$

$$
\begin{aligned}
z^1_1 &= h(a^1_1) \\
z^1_2 &= h(a^1_2) \\
z^1_2 &= h(a^1_3)
\end{aligned}
$$

---
#### ３層ニューラルネットワーク(隱れ層②)

隱れ層①からの入力から隱れ層②の出力値を計算。

$$
\begin{aligned}
a^2_1 &= w^2_{11} z^1_1 + w^2_{12} z^1_2 + w^2_{13} z^1_3 + b^2_1 \\
a^2_2 &= w^2_{21} z^1_2 + w^2_{22} z^1_2 + w^2_{23} z^1_3 + b^2_2
\end{aligned}
$$

$$
\begin{aligned}
z^2_1 = h(a^2_1) \\
z^2_2 = h(a^2_2)
\end{aligned}
$$

---
#### ３層ニューラルネットワーク(出力層)

隱れ層②からの入力受けて出力層での結果を計算

$$
\begin{aligned}
a^3_1 &= w^3_{11} z^2_1 + w^3_{12} z^2_2 + b^1_1 \\
a^3_2 &= w^3_{21} z^2_1 + w^3_{22} z^2_2 + b^1_2
\end{aligned}
$$

活性化関数は、隱れ層と 異なる関数$f(x)$を使用。

$$
\begin{aligned}
y_1 = f(a^3_1) \\
y_2 = f(a^3_2)
\end{aligned}
$$

$y_1$と$y_2$がニューラルネットワークの出力値となる。

---
#### ３層ニューラルネットワーク(まとめ)

![50% center ３層ニューラルネットワークのイメージ](./02_neural_network/neural_network.png)

計算内容が難しそうに見えるが、以下３つを繰り返しているだけ。

- １つ前層の出力値（入力値）に重みを掛ける
- 重みを掛けたものと閾値の総和を求める
- 活性化関数で出力値を求める。

---
### 出力層で使用する活性化関数

出力層で使用する活性化関数は、ニューラルネットワークを使用する目的に合わせて使い分ける。

- 回帰問題
  - 入力データから数値の予測を行う問題。
  - 恒等関数を出力層の活性化関数として使う。
- 分類問題
  - データがどのクラスに所属するかを判断する問題。
  - ソフトマックス関数を出力層の活性化関数として使う。

---
#### 出力層(恒等関数)

恒等関数は、入力値がそのまま出力値となる関数。

$$
f(x) = x
$$

回帰問題であれば、出力値がそのまま予測数値となる。

---
##### 出力層(恒等関数：実装)

恒等関数を実装する。
```python
def identity_function(x):
    y = x
    return y
```

---
#### 出力層(ソフトマックス関数)

ソフトマックス関数は、全ての入力値の総和を分母に、特定の入力値の分子として、特定の入力値がもつ割合を求める関数。

$$
f(x) = \frac {exp(a_k)}{\sum^n_{i=1} exp(a_i)}
$$

分類問題であれば、出力結果だと予想される割合として解釈できる。

---
##### 出力層(ソフトマックス関数：改善)

$exp(a_k)$が極めて大きな値になり、コンピュータでの計算ではオーバーフローを起こす可能性がある。
オバーフローを起こすことへの対応を行いつつ、同じ結果を得ることができる範囲で以下のように数式を変形させる。

$$
f(x) = \frac {exp(a_k - C')}{\sum^n_{i=1} exp(a_i - C')}
$$

$C'$には$a_k$のなかで最大の値を使用するのが一般的。

---
##### 出力層(ソフトマックス関数：実装)

ソフトマックス関数を実装する。
```python
def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a - c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / np.sum(sum_exp_a)
    return y
```

---
### 行列式

簡単なニューラルネットワークでも、ここに計算式を記述していくと大変そうに見えてた。
行列式の力を借りて、計算式をシンプルにする。

使うのは３つだけ。

- 行列
- 足し算
- 掛け算(内積)
- 転置

---
#### 行列式（行列）

まずは、１次元行列、２次元行列を導入する。

* １次元行列(プログラム的には１次元配列)

$$
X = 
\left(
\begin{array}{cccc}
x_1 & x_2 & \cdots & x_n
\end{array}
\right)
$$

* ２次元行列(プログラム的には２次元配列)

$$
W = 
\left(
\begin{array}{cccc}
w_{11} & w_{21} & \cdots & w_{n1} \\
w_{12} & w_{22} & \cdots & w_{n2} \\
\vdots & \vdots & \ddots& \vdots\\ 
w_{m1} & w_{2m} & \cdots & w_{nm} \\
\end{array}
\right)
$$

---
#### 行列式(足し算)

* 行列と行列の足し算(素直に足すだけ)

$$
\left( \begin{array}{r} 1 \\ 2 \\ 3 \\ 4 \end{array} \right) + 
\left( \begin{array}{r} 5 \\ 6 \\ 7 \\ 8 \end{array} \right) = 
\left( \begin{array}{r} 1 + 5 \\ 2 + 6 \\ 3 + 7 \\ 4 + 8 \end{array} \right) = 
\left( \begin{array}{r} 6 \\ 8 \\ 10 \\ 12 \end{array} \right)
$$

---
#### 行列式（掛け算：内積）

１次元行列と２次元行列の内積

$$
\begin{aligned}
\text{XW}
&= 
\left(
\begin{array}{rr}
x_1 & x_2 
\end{array}
\right) 
\left(
\begin{array}{rrr}
w_{11} & w_{21} & w_{31} \\
w_{12} & w_{22} & w_{32}
\end{array}
\right)
\\
&
=
\left( 
\begin{array}{rrr}
a_1 & a_2 & a_3
\end{array}
\right) 
\end{aligned}
$$

$a_1$,$a_2$,$a_3$の内容
$$
\begin{aligned}
a_1 = x_1 * w_{11} + x_2 * w_{12} \\
a_2 = x_1 * w_{21} + x_2 * w_{22} \\
a_3 = x_1 * w_{31} + x_2 * w_{32}
\end{aligned}
$$
---
#### 行列式(転置)

計算式として必要ではないけれども、紙面の都合上導入。

掛け算（内積）が分かりやすくなる副作用もある。

---
### ３層ニューラルネットワーク②

![70% center ３層ニューラルネットワークのイメージ](./02_neural_network/neural_network.png)

入力信号が２つ、隱れ層を２層、出力信号が２つの簡単なニューラルネットワークを行列式で表現していく。

---
#### ３層ニューラルネットワーク②-1

入力信号は１次元行列
$$
X = \left( \begin{array}{rr} x_1 & x_2 \end{array} \right)
$$

入力層と隱れ層①間の重みは２次元配列
$$
W^1 = 
\left(
\begin{array}{lll}
w^1_{11} & w^1_{21} & w^1_{31}\\
w^1_{12} & w^1_{22} & w^1_{32}
\end{array}
\right)
$$

隱れ層①の閾値は１次元配列
$$
B^1 = \left( \begin{array}{lll} b^1_1 & b^1_2 & b^1_3 \end{array} \right)
$$

---
#### ３層ニューラルネットワーク②-2

隱れ層①の入力信号の総和は下式。
$$
\begin{aligned}
A^1 &= X W^1 + B^1 \\
&=
\left( \begin{array}{rr} x_1 & x_2 \end{array} \right)
\left(
\begin{array}{ll}

w^1_{11} & w^1_{12} \\
w^1_{21} & w^1_{22} \\
w^1_{31} & w^1_{32}
\end{array}
\right)^T
+
\left( \begin{array}{l} 
b^1_1 \\
b^1_2 \\
b^1_3 
\end{array} \right) ^T
\\
&=
\left(
\begin{array}{lll}
x_1 * w^1_{11} + x_2 * w^1_{12} + b^1_1 \\
x_1 * w^1_{21} + x_2 * w^1_{22} + b^1_2 \\
x_1 * w^1_{31} + x_2 * w^1_{32} + b^1_3
\end{array}
\right)^T
\\
&=
\left( \begin{array}{rrr}
a^1_1 & a^1_2 & a^1_3
\end{array} \right)
\end{aligned}
$$

---
#### ３層ニューラルネットワーク②-3
隱れ層①の出力信号は下式。

$$
\begin{aligned}
Z^1 &= h(A^1)
\\
&=
\left( \begin{array}{rrr}
h(a^1_1) & h(a^1_2) & h(a^1_3)
\end{array} \right)
\\ &=
\left( \begin{array}{lll} z^1_1 & z^1_2 & z^1_3 \end{array} \right)
\end{aligned}
$$

---
#### ３層ニューラルネットワーク②-3

隱れ層②や出力層についても同じように計算する。
式を記載すると下式のようになる。

層| 計算式
---|---
入力層 | $X$
隱れ層① | $\begin{aligned}A^1 &= X W^1 + B^1 \\Z^1 &= h(A^1)\end{aligned}$
隱れ層② | $\begin{aligned}A^2 &= Z^1 W^2 + B^2 \\Z^2 &= h(A^2)\end{aligned}$
出力層 | $\begin{aligned}A^3 &= Z^2 W^3 + B^3 \\Y &= f(A^3)\end{aligned}$

---
#### ３層ニューラルネットワーク②-4

ニューラルネットワークを整理すると２種類の計算式で成り立っている。

計算内容 | 計算式 | 名前
---|---|---
入力信号の総和 | $A = X W +B$ | Affine
活性化関数 | $Y = f(A)$ | ReLU <br/> Softmax

![100% center 行列式を導入したニューラルネットワーク](./02_neural_network/neural_network2.png)

---
### 文字認識

ニューラルネットワークを使った文字認識を




こんなニューラルネットワークを想定するよ。
入力のmnistのデータつかうよ。
ニューラルネットワークでのhello worldレベル。らしい。

---
#### 文字認識(ニューラルネットワークの構造)

あ

---
#### 文字認識(計算式)

層名|計算式
:--|:--
入力層|$X$
隱れ層①|$\begin{aligned}A^1 &= X W^1 + B^1 \\Z^1 &= \text{Relu}(A^1)\end{aligned}$
隱れ層②|$\begin{aligned}A^2 &= Z^1 W^2 + B^2 \\Z^2 &= \text{Relu} (A^2)\end{aligned}$
出力層|$\begin{aligned}A^3 &= Z^2 W^3 + B^3 \\Y &= \text{softmax} (A^3)\end{aligned}$

---
#### ニューラルネットワークで文字認識(実装)

あああ
```python

```

---
### まとめ

まとめを書く。


---
## ニューラルネットワークの学習

---
### まとめ

まとめを書く。

---
## 逆誤差伝播方式

---
### まとめ

まとめを書く。

---
## 学習に関するテクニック

---
### まとめ

まとめを書く。

---
## 畳み込みニューラルネットワーク

---
### まとめ

まとめを書く。

---
## ディプラーニング

---
### まとめ

まとめを書く。
